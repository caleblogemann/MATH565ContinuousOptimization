\documentclass[11pt, oneside]{article}
\usepackage[letterpaper, margin=2cm]{geometry}
\usepackage{MATH565}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 565 Continuous Optimization \\
Midterm Exam
}}

%\lstinputlisting[language=Python]{H01_23.m}
\begin{enumerate}
  \item % #1
    \begin{enumerate}
      \item[(a)] % Done
        Derive an expression for the gradient $\v{\nabla f}$.

        In order to derive an expression for $\v{\nabla f}$ we must compute all
        the partial derivatives $\pd{f}{x_k}$ and $\pd{f}{y_k}$ for all
        $1 \le k \le N$.

        In order to compute these partial derivatives, I will first rewrite $f$
        in a way that seperates the $x_k$ and $y_k$ terms.
        \begin{align*}
          f(\v{x}) = \p{(x_k - x_k)^2 + (y_k - y_k)^2 - d_{kk}^2}^2
          + \sum*{\substack{j = 1 \\ j \neq k}}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}^2} \\
          + \sum*{\substack{i = 1 \\ i \neq k}}{N}{\p{(x_i - x_k)^2 + (y_i - y_k)^2 - d_{ik}^2}^2}
          + \sum*{\substack{j = 1 \\ j \neq k}}{N}{\sum*{\substack{i = 1 \\ i \neq k}}{N}{\p{(x_i - x_j)^2 + (y_i - y_j)^2 - d_{ik}^2}^2}}
        \end{align*}
        By noting that $(x_i - x_k)^2 = (x_k - x_i)^2$ and that $d_{ik} = d_{ki}$,
        this can be simplified to
        \begin{align*}
          f(\v{x}) = d_{kk}^4
          + 2\sum*{\substack{j = 1 \\ j \neq k}}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}^2}
          + \sum*{\substack{j = 1 \\ j \neq k}}{N}{\sum*{\substack{i = 1 \\ i \neq k}}{N}{\p{(x_i - x_j)^2 + (y_i - y_j)^2 - d_{ik}^2}^2}}.
        \end{align*}
        Now the partial derivatives of this can be taken easily note that the
        first and last terms don't contain $x_k$ or $y_k$.
        The partial derivatives are
        \begin{align*}
          \pd{f}{x_k} &= 8 \sum{\substack{j = 1 \\ j \neq k}}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(x_k - x_j)} \\
          \pd{f}{y_k} &= 8 \sum{\substack{j = 1 \\ j \neq k}}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(y_k - y_j)}
        \end{align*}
        When $j = k$ the terms of these sums are zero, so these can also be expressed as
        \begin{align*}
          \pd{f}{x_k} &= 8 \sum{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(x_k - x_j)} \\
          \pd{f}{y_k} &= 8 \sum{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(y_k - y_j)}.
        \end{align*}

        Now the gradient of $f$ can be written using these partial derivatives, as follows
        \[
          \v{\nabla f}(\v{x}) =
          \begin{bmatrix}
            \pd{f}{x_1} \\
            \pd{f}{y_1} \\
            \pd{f}{x_2} \\
            \pd{f}{y_2} \\
            \vdots \\
            \pd{f}{x_N} \\
            \pd{f}{y_N} \\
          \end{bmatrix}
        \]

      \item[(b)] % Done
        Derive an expression for the Hessian $\M{\nabla^2 f}$.

        The Hessian for this function requires finding all possible partial
        second derivatives.
        There are several possible different partial second derivatives.
        They are $\pd[2]{f}{x_k}$, $\pd[2]{f}{y_k}$,
        $\mpd[2]{f}{\partial x_k \partial y_k}$,
        $\mpd[2]{f}{\partial x_k \partial x_i}$,
        $\mpd[2]{f}{\partial y_k \partial y_i}$, and 
        $\mpd[2]{f}{\partial x_k \partial y_i}$, where $k \neq i$.

        First I will compute $\pd[2]{f}{x_k}$ and $\pd[2]{f}{y_k}$.
        \begin{align*}
          \pd[2]{f}{x_k} &= \pda{8 \sum*{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(x_k - x_j)}}{x_k} \\
          &= \pda{8 \sum*{j = 1}{N}{(x_k - x_j)^3 + \p{(y_k - y_j)^2 - d_{kj}^2}(x_k - x_j)}}{x_k} \\
          &= 8 \sum*{j = 1}{N}{\pda{(x_k - x_j)^3}{x_k} + \p{(y_k - y_j)^2 - d_{kj}^2}\pda{(x_k - x_j)}{x_k}} \\
          &= 8 \sum*{j = 1}{N}{3(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2} \\
          \pd[2]{f}{y_k} &= \pda{8 \sum*{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(y_k - y_j)}}{y_k} \\
          &= 8 \sum*{j = 1}{N}{(x_k - x_j)^2 + 3(y_k - y_j)^2 - d_{kj}^2} \\
        \end{align*}

        Next I will compute $\mpd[2]{f}{\partial x_k \partial y_k}$.
        \begin{align*}
          \mpd[2]{f}{\partial x_k \partial y_k} &= \pda{8 \sum*{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(x_k - x_j)}}{y_k} \\
          &= 8 \sum*{j = 1}{N}{(x_k - x_j)\pda{(y_k - y_j)^2}{y_k}} \\
          &= 16 \sum*{j = 1}{N}{(x_k - x_j)(y_k - y_j)}
        \end{align*}

        Thirdly I will compute $\mpd[2]{f}{\partial x_k \partial x_i}$ and
        $\mpd[2]{f}{\partial y_k \partial y_i}$, for $i \neq k$.
        \begin{align*}
          \mpd[2]{f}{\partial x_k \partial x_i} &= \pda{8 \sum*{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(x_k - x_j)}}{x_i} \\
          &= \pda{8\p{(x_k - x_i)^2 + (y_k - y_i)^2 - d_{ki}^2}(x_k - x_i)}{x_i} \\
          &= \pda{8(x_k - x_i)^3 +8\p{(y_k - y_i)^2 - d_{ki}^2}(x_k - x_i)}{x_i} \\
          &= -24(x_k - x_i)^2 - 8\p{(y_k - y_i)^2 - d_{ki}^2} \\
          &= -8\p{(x_k - x_i)^2 + (y_k - y_i)^2 - d_{ki}^2}
          \mpd[2]{f}{\partial y_k \partial y_i} &= \pda{8 \sum*{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(y_k - y_j)}}{y_i} \\
          &= \pda{8 \p{(x_k - x_i)^2 + (y_k - y_i)^2 - d_{ki}^2}(y_k - y_i)}{y_i} \\
          &= \pda{8 \p{(x_k - x_i)^2 - d_{ki}^2}(y_k - y_i)+ (y_k - y_i)^3}{y_i} \\
          &= -8 \p{(x_k - x_i)^2 + 3(y_k - y_i)^2 - d_{ki}^2}{y_i} \\
        \end{align*}

        Lastly I will compute $\mpd[2]{f}{\partial x_k \partial y_i}$ for
        $i \neq k$.
        Note that this is equivalent to $\mpd[2]{f}{\partial y_i \partial x_k}$.
        \begin{align*}
          \mpd[2]{f}{\partial x_k \partial y_i} &= \pda{8 \sum*{j = 1}{N}{\p{(x_k - x_j)^2 + (y_k - y_j)^2 - d_{kj}^2}(x_k - x_j)}}{y_i} \\
          &= \pda{8 \p{(x_k - x_i)^2 + (y_k - y_i)^2 - d_{ki}^2}(x_k - x_i)}{y_i} \\
          &= -16(x_k - x_i)(y_k - y_i) \\
        \end{align*}

        Now with all these partial derivatives the Hessian can be shown as a
        matrix of partial derivatives.
        \begin{align*}
          \M{\nabla^2 f} =
          \begin{bmatrix}
            \pd[2]{f}{x_1} & \mpd[2]{f}{\partial x_1 \partial y_1} & \cdots & \mpd[2]{f}{\partial x_1 \partial x_N} & \mpd[2]{f}{\partial x_1 \partial y_N} \\
            \mpd[2]{f}{\partial y_1 \partial x_1} & \pd[2]{f}{y_1} & \cdots & \mpd[2]{f}{\partial y_1 \partial x_N} & \mpd[2]{f}{\partial y_1 \partial y_N} \\
            \vdots & \vdots & \ddots & \vdots & \vdots \\
            \mpd[2]{f}{\partial x_N \partial x_1} & \mpd[2]{f}{\partial x_N \partial y_1} & \cdots &  \pd[2]{f}{x_N} & \mpd[2]{f}{\partial x_N \partial y_N} \\
            \mpd[2]{f}{\partial y_N \partial x_1} & \mpd[2]{f}{\partial y_N \partial y_1} & \cdots &  \mpd[2]{f}{\partial y_N \partial x_N} & \pd[2]{f}{y_N} \\
          \end{bmatrix}
        \end{align*}

      \item[(c)]
        Prove that the Hessian, $\M{\nabla^2 f}$ is symmetrix positive definite
        for all $\v{x} \in \RR^{2N}$.

        \begin{proof}
          Cleary the Hessian is symmetric as all of the partial derivatives
          are symmetric.
        \end{proof}
    \end{enumerate}

  \item % #2
    \begin{enumerate}
      \item[(a)]
        \lstinputlisting[language=Python]{trustRegionMethod.py}

      \item[(b)]
        \lstinputlisting[language=Python]{doglegMethod.py}

      \item[(c)]
        \lstinputlisting[language=Python]{midtermExam_2.py}
    \end{enumerate}

  
  \item % #3

\end{enumerate}
\end{document}
