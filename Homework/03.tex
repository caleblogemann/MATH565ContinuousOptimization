\documentclass[11pt, oneside]{article}
\usepackage[letterpaper, margin=2cm]{geometry}
\usepackage{MATH565}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 565 Continuous Optimization \\
Homework 3
}}

%\lstinputlisting[language=Python]{H01_23.m}
\begin{enumerate}
  \item % #1
    Page 100: Problem 4.9. \\
    Derive the solution of the two-dimensional subspace minimization problem in
    the case where $B$ is positive definite.

    \begin{proof}
      
    \end{proof}

  \item % #2 Done
    Page 100: Problem 4.10. \\
    Show that if $B$ is any symmetric matrix, then there exists $\lambda \ge 0$
    such that $B + \lambda I$ is positive definite.

    \begin{proof}
      Let $B$ be a symmetric matrix.
      Since $B$ is symmetric all of the eigenvalues of $B$ are real.
      If all of the eigenvalues of $B$ are positive, then $B$ is already
      positive definite.
      In this case, $B = B + 0 I$ is positive definite.
      Therefore consider when $B$ has some negative eigenvalues.
      Let $\mu_1$ be the most negative eigenvalue, that is $\mu_1 \le \mu_i$ for any
      eigenvalue, $\mu_i$, of $B$.
      I will let $\lambda = -\mu_1 + 1$.
      I now claim that $B + \lambda I$ is positive definite.
      To see this note that the eigenvectors of $B$ can form an orthonormal basis
      of $\RR^N$, when $B \in \RR^{N \times N}$.
      Let $\set{v_i}$ denote this basis and consider that any $x \in \RR^N$ can
      be expressed as $x = \sum{i = 1}{N}{a_i v_i}$.
      Now consider $x^T \p{B + \lambda I} x$.
      \begin{align*}
        x^T \p{B + \lambda I} x &= \sum{i = 1}{N}{a_i v_i^T} \p{B + \lambda I} \sum{j = 1}{N}{a_j v_j} \\
        &= \sum{i = 1}{N}{\sum{j = 1}{N}{a_i a_j v_i^T \p{B + \lambda I}v_j}} \\
        &= \sum{i = 1}{N}{\sum{j = 1}{N}{a_i a_j \p{v_i^T B v_j + \lambda v_i^T v_j}}} \\
        &= \sum{i = 1}{N}{\sum{j = 1}{N}{a_i a_j \p{\mu_j v_i^T v_j + \lambda v_i^T v_j}}}
        \intertext{Since $v_i^T v_j = 0$ for all $i \neq j$}
        &= \sum{i = 1}{N}{a_i^2 \p{\mu_j v_i^T v_i + \lambda v_i^T v_i}}
        &= \sum{i = 1}{N}{a_i^2 \p{\mu_i \norm{v_i}^2 + \lambda \norm{v_i}^2}} \\
        &= \sum{i = 1}{N}{a_i^2 \p{\mu_i + \lambda}}
        \intertext{Note that $\mu_i + \lambda > 0$ for any eigenvalue $\mu_i$ as
          $\lambda + \mu_1 = 1$ and $\mu_1 \le \mu_i$ for all eigenvalues, therefore}
        x^T \p{B + \lambda I} x &> 0
      \end{align*}
      This shows that $B + \lambda I$ is positive definite.
    \end{proof}

  \item % #3 Done
    Implement the linear conjugate gradient method in \MATLAB or \PYTHON.

    The following function implements the linear conjugate gradient method in
    \PYTHON.
    \lstinputlisting[language=Python]{linearConjugateGradient.py}

  \item % #4
    Page 133: Problem 5.1. (Use your method from the previous problem). \\
    Implement Algorithm 5.2 and use it to solve linear systems in which $A$
    is the Hilbert matrix, whose elements $A_{i, j} = 1/(i + j - 1)$.
    Set the right-hand-side to $b = (1, 1, \ldots, 1)^T$ and the initial point
    to $x_0 = 0$.
    Try dimensions $n = 5, 8, 12, 20$ and report the number of iterations
    required to reduce the residual below $10^{-6}$.

    The following script uses the linear conjugate gradient method to solve the
    given linear system.
    \lstinputlisting[language=Python]{03_4.py}

    The required number of iterations to achieve error less than $10^{-6}$ is
    shown in the table below for different size matrices.
    \begin{center}
      \begin{tabular}
        \toprule
           n & Number of Iterations \\
        \midrule
           5 &  \\
           8 &  \\
          12 &  \\
          20 &  \\
        \bottomrule
      \end{tabular}
    \end{center}

  \item % #5 Done
    Page 133: Problem 5.2. \\
    Show that if the nonzero vectors $p_0, p_1, \ldots, p_l$ satisfy (5.5),
    where $A$ is symmetric and positive definite, then these vectors are
    linearly independent.
    (This result implies that $A$ has a most $n$ conjugate directions.)

    \begin{proof}
      Let $A$ be symmetric and positive definite and
      let $p_0, p_1, \ldots, p_l$ be A-conjugate, that is
      \[
        p_i^T A p_j = 0 \qquad \forall i \neq j.
      \]
      Consider a set of constants $c_0, c_1, \ldots, c_l$, such that
      \[
        \sum{i = 0}{l}{c_i p_i} = 0
      \]
      The set of vectors $\set{p_i}$ are linearly independent if $c_i = 0$ for
      all $i$.
      Consider the following
      \begin{align*}
        0 &= \sum{i = 0}{l}{c_i p_i^T} A \sum{j = 0}{l}{c_j p_j} \\
        0 &= \sum{i = 0}{l}{\sum{j = 0}{l}{c_i c_j  p_i^T A p_j}}
        \intertext{Since these vectors are A-conjugate}
        0 &= \sum{i = 0}{l}{c_i^2 p_i^T A p_i}.
        \intertext{Note that since $A$ is positive definite $p_i^T A p_i > 0$
          for all $i$, and since $c_i^2 \ge 0$ this implies that}
        0 &= c_i
      \end{align*}
      This shows that $p_0, p_1, \ldots, p_l$ are linearly independent.
      Therefore any set of A-conjugate vectors must also be linearly
      independent.
      Since a set of linearly independent vectors can be at most of size $n$,
      this implies that a set of A-conjugate vectors can be at most of size $n$.
    \end{proof}

  \item % #6
    Let $n = N^2$.
    Downlonad the \MATLAB file CreateA.m from the course website.
    The correct syntax for calling this code is
    \[
      A = CreateA(N);
    \]
    This creates a matrix of size $N^2 \times N^2$.

    Apply your conjugate gradient method to this problem for various $N$.
    Make a table that records the number of iterations required to achieve a
    reasonable tolerance for $N = 10, 20, 40, 80, 160, 320$.
    You should use the same tolerance in each case.
    How does the number of iterations scale with N?
    What does this tell you about the condition number of $A$ as $N$ varies?

    \lstinputlisting[language=Python]{03_6.py}

    For this problem, I used $b = \br{1, 1, \ldots, 1}^T$ and $x_0 = 0$.
    The error tolerance was $10^{-6}$.
    \begin{center}
      \begin{tabular}
        \toprule
        N & number of iterations \\
        \midrule
        10 & 15 \\
        20 & 35 \\
        40 & 73 \\
        80 & 149 \\
        160 & 302 \\
        320 & \\
        \bottomrule
      \end{tabular}
    \end{center}
    This sho

\end{enumerate}
\end{document}
